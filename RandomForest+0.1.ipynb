{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def g0(Z_t, C):\n",
    "    return np.cos(Z_t @ C)# 假设是这个函数,cosine\n",
    "\n",
    "def make_odefunc(beta_0, Z_interp, C):\n",
    "    def odefunc(t, X_flat):\n",
    "        X = X_flat.reshape(1, -1)\n",
    "        Z_t = Z_interp(t).reshape(1, -1)\n",
    "        return (X @ beta_0 + g0(Z_t, C)).flatten()\n",
    "    return odefunc\n",
    "\n",
    "def find_best_alpha(Z, Y, alphas, cv):\n",
    "    \"\"\"\n",
    "    Use GridSearchCV to find the best alpha for Kernel Ridge Regression.\n",
    "    \"\"\"\n",
    "    model = KernelRidge(kernel='rbf')\n",
    "    param_grid = {'alpha': alphas}\n",
    "    grid = GridSearchCV(model, param_grid, cv=cv, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid.fit(Z, Y)\n",
    "\n",
    "    best_alpha = grid.best_params_['alpha']\n",
    "    best_mse = -grid.best_score_  # 注意：scoring 是 neg_MSE，要取负号\n",
    "    return best_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(200):\n",
    "  Z = np.random.randn(T, nu)\n",
    "  Z_interp = interp1d(t_grid, Z, axis=0, kind='cubic', fill_value=\"extrapolate\")  # Interpolate Z(t) for continuous time within the solver\n",
    "  \n",
    "  X0 = np.zeros(p)\n",
    "  \n",
    "  C = np.random.randn(nu, p)\n",
    "  \n",
    "  odefunc = make_odefunc(beta_0, Z_interp, C)\n",
    "  sol = solve_ivp(odefunc, [0, 1], X0, t_eval=t_grid, method='RK45')\n",
    "  X_true = sol.y.T  # (T,p)\n",
    "  \n",
    "  dXdt = np.zeros_like(X_true)\n",
    "  for idx, t in enumerate(t_grid):\n",
    "      X_t = X_true[idx, :].reshape(1, -1)\n",
    "      Z_t = Z[idx, :].reshape(1, -1)\n",
    "      dXdt[idx, :] = (X_t @ beta_0 + g0(Z_t, C)).flatten()\n",
    "      \n",
    "  \n",
    "  #更改了U的噪声强度从0.05到0.1\n",
    "  U = 0.1 * np.random.randn(T, p)  # noise\n",
    "  Y = X_true + U  # observation\n",
    "  \n",
    "  \n",
    "  Q = 20\n",
    "  kf = KFold(n_splits=Q, shuffle=True, random_state=42)\n",
    "  beta_q_list = []\n",
    "  Delta_t = t_grid[1] - t_grid[0]\n",
    "  \n",
    "  \n",
    "  best_alpha = find_best_alpha(Z, Y, alphas = np.logspace(-2,2,20), cv= KFold(n_splits=5, shuffle=True, random_state=42))\n",
    "  \n",
    "  for Iqc, Iq in tqdm(kf.split(range(T)), total=Q, desc=\"Cross-fitting\"):\n",
    "      # Estimate r_hat, g_hat on Iqc (complement of chunk q)\n",
    "      Z_qc, Y_qc = Z[Iqc], Y[Iqc]\n",
    "      Z_q, Y_q = Z[Iq], Y[Iq]\n",
    "      dXdt_q = dXdt[Iq]\n",
    "  \n",
    "      r_hat = KernelRidge(kernel='rbf', alpha=best_alpha)  # Estimate r_hat with kernel ridge regression\n",
    "      r_hat.fit(Z_qc, Y_qc)\n",
    "      r_hat_q = r_hat.predict(Z_q)\n",
    "      \n",
    "      g_hat = RandomForestRegressor(n_estimators=100) #change into random forest regressor\n",
    "                          \n",
    "     \n",
    "      #g_hat = RandomForestRegressor(n_estimators=100)\n",
    "      g_hat.fit(Z_qc, dXdt[Iqc].ravel())\n",
    "      g_hat_q = g_hat.predict(Z_q)\n",
    "  \n",
    "      # Solve for beta_hat using Neyman orthogonal score\n",
    "      d_l = r_hat_q.flatten() - Y_q.flatten()  # shape (len(Iq),)\n",
    "      s_l = np.cumsum(Y_q.flatten()) * Delta_t  # integrate X using observed Y\n",
    "      g_l = np.cumsum(g_hat_q) * Delta_t  # integrate g_hat\n",
    "  \n",
    "      beta_hat_q = np.sum(d_l * (r_hat_q.flatten() - g_l)) / np.sum(d_l * s_l)#这里用r_hat_q估计效果更好，why\n",
    "      beta_q_list.append(beta_hat_q)\n",
    "  \n",
    "  # Average over Q chunks\n",
    "  beta_hat = np.mean(beta_q_list)\n",
    "  \n",
    "  print(\"Estimated beta:\\n\", beta_hat)\n",
    "  beta_forest.append(beta_hat)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
